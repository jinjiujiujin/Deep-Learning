{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN生成手写数字，使用Mnist数据集，pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置数据\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE=32\n",
    "img_size=28*28\n",
    "hidden_size=256\n",
    "latent_size=128\n",
    "EPOCHS=128\n",
    "lr=2e-4 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 全连接层1 输入为img size, 输出为hidden size\n",
    "        self.layer1 = nn.Linear(img_size, hidden_size)\n",
    "        # 激活函数1 LeakyReLU\n",
    "        self.actF1 = nn.LeakyReLU(0.2) # negative_slope=0.2\n",
    "        # 全连接层2 输入为hidden size, 输出为hidden size\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 全连接层3 输入为hidden size, 输出为1\n",
    "        self.layer3 = nn.Linear(hidden_size, 1)\n",
    "        # 激活函数2 sigmoid 将变量映射到区间(0, 1) 常用于二分类问题\n",
    "        self.actF2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # 起始 image维度为[BATCH_SIZE, img_size]\n",
    "        # [BATCH_SIZE, img_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.layer1(images)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.actF1(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.layer2(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.actF1(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, 1]\n",
    "        x = self.layer3(x)\n",
    "        # [BATCH_SIZE, 1] ---> [BATCH_SIZE, 1]\n",
    "        y = self.actF2(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # 全连接层1 输入为latent size, 输出为hidden size\n",
    "        self.layer1 = nn.Linear(latent_size, hidden_size)\n",
    "        # 激活函数1 ReLU\n",
    "        self.actF1 = nn.ReLU()\n",
    "        # 全连接层2 输入为hidden size, 输出为hidden size\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 全连接层3 输入为hidden size, 输出为img size\n",
    "        self.layer3 = nn.Linear(hidden_size, img_size)\n",
    "        # 激活函数2 Tanh https://blog.csdn.net/m0_38065572/article/details/104939343\n",
    "        self.actF2 = nn.Tanh() #mnist是二值图，所以用tanh将生成图片的像素压在[0,1]间\n",
    "    \n",
    "    def forward(self, latents):\n",
    "        # 初始 latent维度为[BATCH_SIZE, latent_size]\n",
    "        # [BATCH_SIZE, latent_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.layer1(latents)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.actF1(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.layer2(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, hidden_size]\n",
    "        x = self.actF1(x)\n",
    "        # [BATCH_SIZE, hidden_size] ---> [BATCH_SIZE, img_size]\n",
    "        x = self.layer3(x)\n",
    "        # [BATCH_SIZE, img_size] ---> [BATCH_SIZE, img_size]\n",
    "        imgs = self.actF2(x)\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, Step 0: real_score 0.500629, fake_score 0.499407\n",
      "Epoch 0, Step 1000: real_score 0.64277, fake_score 0.398773\n",
      "Epoch 1, Step 0: real_score 0.897629, fake_score 0.0830381\n",
      "Epoch 1, Step 1000: real_score 0.679601, fake_score 0.0510367\n",
      "Epoch 2, Step 0: real_score 0.913893, fake_score 0.0903284\n",
      "Epoch 2, Step 1000: real_score 0.969518, fake_score 0.0637543\n",
      "Epoch 3, Step 0: real_score 0.901613, fake_score 0.101226\n",
      "Epoch 3, Step 1000: real_score 0.934983, fake_score 0.131044\n",
      "Epoch 4, Step 0: real_score 0.903144, fake_score 0.0275867\n",
      "Epoch 4, Step 1000: real_score 0.975508, fake_score 0.0902862\n",
      "Epoch 5, Step 0: real_score 0.901234, fake_score 0.0472012\n",
      "Epoch 5, Step 1000: real_score 0.965798, fake_score 0.0441413\n",
      "Epoch 6, Step 0: real_score 0.859261, fake_score 0.0330772\n",
      "Epoch 6, Step 1000: real_score 0.888456, fake_score 0.00643773\n",
      "Epoch 7, Step 0: real_score 0.965684, fake_score 0.0361359\n",
      "Epoch 7, Step 1000: real_score 0.938034, fake_score 0.036914\n",
      "Epoch 8, Step 0: real_score 0.935918, fake_score 0.0335545\n",
      "Epoch 8, Step 1000: real_score 0.912623, fake_score 0.0524213\n",
      "Epoch 9, Step 0: real_score 0.954427, fake_score 0.0354194\n",
      "Epoch 9, Step 1000: real_score 0.996701, fake_score 0.0938355\n",
      "Epoch 10, Step 0: real_score 0.912024, fake_score 0.0600692\n",
      "Epoch 10, Step 1000: real_score 0.956847, fake_score 0.092136\n",
      "Epoch 11, Step 0: real_score 0.966998, fake_score 0.0348748\n",
      "Epoch 11, Step 1000: real_score 0.976287, fake_score 0.0143974\n",
      "Epoch 12, Step 0: real_score 0.959396, fake_score 0.114995\n",
      "Epoch 12, Step 1000: real_score 0.92966, fake_score 0.047149\n",
      "Epoch 13, Step 0: real_score 0.933581, fake_score 0.0512668\n",
      "Epoch 13, Step 1000: real_score 0.96143, fake_score 0.0462426\n",
      "Epoch 14, Step 0: real_score 0.938998, fake_score 0.0498836\n",
      "Epoch 14, Step 1000: real_score 0.988016, fake_score 0.0414409\n",
      "Epoch 15, Step 0: real_score 0.983242, fake_score 0.0191509\n",
      "Epoch 15, Step 1000: real_score 0.988339, fake_score 0.0749821\n",
      "Epoch 16, Step 0: real_score 0.935809, fake_score 0.017545\n",
      "Epoch 16, Step 1000: real_score 0.964936, fake_score 0.0948852\n",
      "Epoch 17, Step 0: real_score 0.972682, fake_score 0.0772927\n",
      "Epoch 17, Step 1000: real_score 0.983321, fake_score 0.0867007\n",
      "Epoch 18, Step 0: real_score 0.973442, fake_score 0.0706153\n",
      "Epoch 18, Step 1000: real_score 0.974352, fake_score 0.0521945\n",
      "Epoch 19, Step 0: real_score 0.918936, fake_score 0.0313979\n",
      "Epoch 19, Step 1000: real_score 0.986325, fake_score 0.0527732\n",
      "Epoch 20, Step 0: real_score 0.962818, fake_score 0.0131649\n",
      "Epoch 20, Step 1000: real_score 0.910944, fake_score 0.0556673\n",
      "Epoch 21, Step 0: real_score 0.947531, fake_score 0.0292607\n",
      "Epoch 21, Step 1000: real_score 0.89665, fake_score 0.152808\n",
      "Epoch 22, Step 0: real_score 0.926567, fake_score 0.026899\n",
      "Epoch 22, Step 1000: real_score 0.949436, fake_score 0.039637\n",
      "Epoch 23, Step 0: real_score 0.945318, fake_score 0.0189165\n",
      "Epoch 23, Step 1000: real_score 0.991675, fake_score 0.0860996\n",
      "Epoch 24, Step 0: real_score 0.923848, fake_score 0.0452509\n",
      "Epoch 24, Step 1000: real_score 0.958745, fake_score 0.0662796\n",
      "Epoch 25, Step 0: real_score 0.977127, fake_score 0.0173519\n",
      "Epoch 25, Step 1000: real_score 0.948962, fake_score 0.0442968\n",
      "Epoch 26, Step 0: real_score 0.96144, fake_score 0.0705601\n",
      "Epoch 26, Step 1000: real_score 0.957734, fake_score 0.105286\n",
      "Epoch 27, Step 0: real_score 0.918335, fake_score 0.0159843\n",
      "Epoch 27, Step 1000: real_score 0.982964, fake_score 0.0878269\n",
      "Epoch 28, Step 0: real_score 0.931964, fake_score 0.113539\n",
      "Epoch 28, Step 1000: real_score 0.907402, fake_score 0.0549578\n",
      "Epoch 29, Step 0: real_score 0.909559, fake_score 0.164465\n",
      "Epoch 29, Step 1000: real_score 0.839538, fake_score 0.0923014\n",
      "Epoch 30, Step 0: real_score 0.895689, fake_score 0.0568971\n",
      "Epoch 30, Step 1000: real_score 0.934853, fake_score 0.0982874\n",
      "Epoch 31, Step 0: real_score 0.945992, fake_score 0.0545447\n",
      "Epoch 31, Step 1000: real_score 0.923714, fake_score 0.17845\n",
      "Epoch 32, Step 0: real_score 0.890701, fake_score 0.049229\n",
      "Epoch 32, Step 1000: real_score 0.898361, fake_score 0.0844738\n",
      "Epoch 33, Step 0: real_score 0.865133, fake_score 0.0425655\n",
      "Epoch 33, Step 1000: real_score 0.838109, fake_score 0.0459554\n",
      "Epoch 34, Step 0: real_score 0.985366, fake_score 0.124375\n",
      "Epoch 34, Step 1000: real_score 0.786117, fake_score 0.0533823\n",
      "Epoch 35, Step 0: real_score 0.923295, fake_score 0.173102\n",
      "Epoch 35, Step 1000: real_score 0.894759, fake_score 0.119852\n",
      "Epoch 36, Step 0: real_score 0.938462, fake_score 0.05683\n",
      "Epoch 36, Step 1000: real_score 0.942401, fake_score 0.217258\n",
      "Epoch 37, Step 0: real_score 0.919778, fake_score 0.0801264\n",
      "Epoch 37, Step 1000: real_score 0.913898, fake_score 0.108439\n",
      "Epoch 38, Step 0: real_score 0.928701, fake_score 0.119415\n",
      "Epoch 38, Step 1000: real_score 0.920722, fake_score 0.147023\n",
      "Epoch 39, Step 0: real_score 0.819711, fake_score 0.083784\n",
      "Epoch 39, Step 1000: real_score 0.948145, fake_score 0.185626\n",
      "Epoch 40, Step 0: real_score 0.86401, fake_score 0.104259\n",
      "Epoch 40, Step 1000: real_score 0.849732, fake_score 0.124903\n",
      "Epoch 41, Step 0: real_score 0.925151, fake_score 0.142004\n",
      "Epoch 41, Step 1000: real_score 0.951714, fake_score 0.162618\n",
      "Epoch 42, Step 0: real_score 0.865067, fake_score 0.0654457\n",
      "Epoch 42, Step 1000: real_score 0.922078, fake_score 0.0831994\n",
      "Epoch 43, Step 0: real_score 0.963245, fake_score 0.0769718\n",
      "Epoch 43, Step 1000: real_score 0.881134, fake_score 0.148989\n",
      "Epoch 44, Step 0: real_score 0.955239, fake_score 0.115536\n",
      "Epoch 44, Step 1000: real_score 0.811242, fake_score 0.210545\n",
      "Epoch 45, Step 0: real_score 0.830479, fake_score 0.0549259\n",
      "Epoch 45, Step 1000: real_score 0.856986, fake_score 0.0744568\n",
      "Epoch 46, Step 0: real_score 0.849854, fake_score 0.115594\n",
      "Epoch 46, Step 1000: real_score 0.888931, fake_score 0.138533\n",
      "Epoch 47, Step 0: real_score 0.896124, fake_score 0.178161\n",
      "Epoch 47, Step 1000: real_score 0.841544, fake_score 0.121193\n",
      "Epoch 48, Step 0: real_score 0.958022, fake_score 0.155401\n",
      "Epoch 48, Step 1000: real_score 0.886654, fake_score 0.165834\n",
      "Epoch 49, Step 0: real_score 0.884641, fake_score 0.106232\n",
      "Epoch 49, Step 1000: real_score 0.850858, fake_score 0.133163\n",
      "Epoch 50, Step 0: real_score 0.969429, fake_score 0.232032\n",
      "Epoch 50, Step 1000: real_score 0.779494, fake_score 0.143528\n",
      "Epoch 51, Step 0: real_score 0.799388, fake_score 0.125932\n",
      "Epoch 51, Step 1000: real_score 0.925247, fake_score 0.213824\n",
      "Epoch 52, Step 0: real_score 0.810049, fake_score 0.110418\n",
      "Epoch 52, Step 1000: real_score 0.886081, fake_score 0.196734\n",
      "Epoch 53, Step 0: real_score 0.857803, fake_score 0.118238\n",
      "Epoch 53, Step 1000: real_score 0.925669, fake_score 0.0751432\n",
      "Epoch 54, Step 0: real_score 0.83604, fake_score 0.173636\n",
      "Epoch 54, Step 1000: real_score 0.882424, fake_score 0.137122\n",
      "Epoch 55, Step 0: real_score 0.843467, fake_score 0.0864308\n",
      "Epoch 55, Step 1000: real_score 0.865027, fake_score 0.162421\n",
      "Epoch 56, Step 0: real_score 0.837627, fake_score 0.0950009\n",
      "Epoch 56, Step 1000: real_score 0.874511, fake_score 0.173954\n",
      "Epoch 57, Step 0: real_score 0.88545, fake_score 0.158537\n",
      "Epoch 57, Step 1000: real_score 0.813222, fake_score 0.0679175\n",
      "Epoch 58, Step 0: real_score 0.893645, fake_score 0.120497\n",
      "Epoch 58, Step 1000: real_score 0.865321, fake_score 0.120554\n",
      "Epoch 59, Step 0: real_score 0.83725, fake_score 0.10957\n",
      "Epoch 59, Step 1000: real_score 0.810903, fake_score 0.126706\n",
      "Epoch 60, Step 0: real_score 0.926567, fake_score 0.176087\n",
      "Epoch 60, Step 1000: real_score 0.878179, fake_score 0.14952\n",
      "Epoch 61, Step 0: real_score 0.756311, fake_score 0.110493\n",
      "Epoch 61, Step 1000: real_score 0.879328, fake_score 0.111078\n",
      "Epoch 62, Step 0: real_score 0.917485, fake_score 0.151805\n",
      "Epoch 62, Step 1000: real_score 0.88129, fake_score 0.166624\n",
      "Epoch 63, Step 0: real_score 0.914103, fake_score 0.193774\n",
      "Epoch 63, Step 1000: real_score 0.855516, fake_score 0.0633841\n",
      "Epoch 64, Step 0: real_score 0.888906, fake_score 0.171328\n",
      "Epoch 64, Step 1000: real_score 0.813391, fake_score 0.106043\n",
      "Epoch 65, Step 0: real_score 0.910538, fake_score 0.157475\n",
      "Epoch 65, Step 1000: real_score 0.803283, fake_score 0.0720788\n",
      "Epoch 66, Step 0: real_score 0.907929, fake_score 0.175804\n",
      "Epoch 66, Step 1000: real_score 0.880577, fake_score 0.267298\n",
      "Epoch 67, Step 0: real_score 0.801073, fake_score 0.100206\n",
      "Epoch 67, Step 1000: real_score 0.866415, fake_score 0.07877\n",
      "Epoch 68, Step 0: real_score 0.863623, fake_score 0.229181\n",
      "Epoch 68, Step 1000: real_score 0.906635, fake_score 0.183301\n",
      "Epoch 69, Step 0: real_score 0.854725, fake_score 0.0751865\n",
      "Epoch 69, Step 1000: real_score 0.848403, fake_score 0.0889254\n",
      "Epoch 70, Step 0: real_score 0.779609, fake_score 0.168117\n",
      "Epoch 70, Step 1000: real_score 0.879404, fake_score 0.22677\n",
      "Epoch 71, Step 0: real_score 0.830766, fake_score 0.132476\n",
      "Epoch 71, Step 1000: real_score 0.889474, fake_score 0.153382\n",
      "Epoch 72, Step 0: real_score 0.861256, fake_score 0.181608\n",
      "Epoch 72, Step 1000: real_score 0.852795, fake_score 0.126188\n",
      "Epoch 73, Step 0: real_score 0.861462, fake_score 0.0994064\n",
      "Epoch 73, Step 1000: real_score 0.889671, fake_score 0.170177\n",
      "Epoch 74, Step 0: real_score 0.896442, fake_score 0.145419\n",
      "Epoch 74, Step 1000: real_score 0.852848, fake_score 0.0977973\n",
      "Epoch 75, Step 0: real_score 0.890285, fake_score 0.181729\n",
      "Epoch 75, Step 1000: real_score 0.828326, fake_score 0.121543\n",
      "Epoch 76, Step 0: real_score 0.852296, fake_score 0.121984\n",
      "Epoch 76, Step 1000: real_score 0.914808, fake_score 0.234584\n",
      "Epoch 77, Step 0: real_score 0.926885, fake_score 0.171261\n",
      "Epoch 77, Step 1000: real_score 0.86891, fake_score 0.255304\n",
      "Epoch 78, Step 0: real_score 0.775657, fake_score 0.121578\n",
      "Epoch 78, Step 1000: real_score 0.807027, fake_score 0.258113\n",
      "Epoch 79, Step 0: real_score 0.889066, fake_score 0.299675\n",
      "Epoch 79, Step 1000: real_score 0.906057, fake_score 0.207771\n",
      "Epoch 80, Step 0: real_score 0.80549, fake_score 0.159188\n",
      "Epoch 80, Step 1000: real_score 0.808134, fake_score 0.138124\n",
      "Epoch 81, Step 0: real_score 0.938087, fake_score 0.148043\n",
      "Epoch 81, Step 1000: real_score 0.834355, fake_score 0.170012\n",
      "Epoch 82, Step 0: real_score 0.834401, fake_score 0.228234\n",
      "Epoch 82, Step 1000: real_score 0.849354, fake_score 0.126615\n",
      "Epoch 83, Step 0: real_score 0.850875, fake_score 0.205159\n",
      "Epoch 83, Step 1000: real_score 0.771812, fake_score 0.152051\n",
      "Epoch 84, Step 0: real_score 0.779878, fake_score 0.178557\n",
      "Epoch 84, Step 1000: real_score 0.871785, fake_score 0.175967\n",
      "Epoch 85, Step 0: real_score 0.809756, fake_score 0.116759\n",
      "Epoch 85, Step 1000: real_score 0.873863, fake_score 0.310038\n",
      "Epoch 86, Step 0: real_score 0.833918, fake_score 0.122394\n",
      "Epoch 86, Step 1000: real_score 0.853071, fake_score 0.16172\n",
      "Epoch 87, Step 0: real_score 0.857765, fake_score 0.10353\n",
      "Epoch 87, Step 1000: real_score 0.841842, fake_score 0.0517851\n",
      "Epoch 88, Step 0: real_score 0.832693, fake_score 0.182945\n",
      "Epoch 88, Step 1000: real_score 0.940755, fake_score 0.255667\n",
      "Epoch 89, Step 0: real_score 0.818535, fake_score 0.0996715\n",
      "Epoch 89, Step 1000: real_score 0.892206, fake_score 0.159046\n",
      "Epoch 90, Step 0: real_score 0.847594, fake_score 0.0931012\n",
      "Epoch 90, Step 1000: real_score 0.900223, fake_score 0.131072\n",
      "Epoch 91, Step 0: real_score 0.88049, fake_score 0.17679\n",
      "Epoch 91, Step 1000: real_score 0.876154, fake_score 0.129207\n",
      "Epoch 92, Step 0: real_score 0.902411, fake_score 0.142821\n",
      "Epoch 92, Step 1000: real_score 0.783991, fake_score 0.144662\n",
      "Epoch 93, Step 0: real_score 0.851112, fake_score 0.0936696\n",
      "Epoch 93, Step 1000: real_score 0.858642, fake_score 0.138294\n",
      "Epoch 94, Step 0: real_score 0.818062, fake_score 0.155741\n",
      "Epoch 94, Step 1000: real_score 0.837971, fake_score 0.15242\n",
      "Epoch 95, Step 0: real_score 0.849819, fake_score 0.108654\n",
      "Epoch 95, Step 1000: real_score 0.802296, fake_score 0.121142\n",
      "Epoch 96, Step 0: real_score 0.789711, fake_score 0.161234\n",
      "Epoch 96, Step 1000: real_score 0.836144, fake_score 0.260023\n",
      "Epoch 97, Step 0: real_score 0.878412, fake_score 0.172793\n",
      "Epoch 97, Step 1000: real_score 0.857722, fake_score 0.26022\n",
      "Epoch 98, Step 0: real_score 0.787665, fake_score 0.103179\n",
      "Epoch 98, Step 1000: real_score 0.844088, fake_score 0.172951\n",
      "Epoch 99, Step 0: real_score 0.905594, fake_score 0.229465\n",
      "Epoch 99, Step 1000: real_score 0.921182, fake_score 0.181941\n",
      "Epoch 100, Step 0: real_score 0.811532, fake_score 0.123864\n",
      "Epoch 100, Step 1000: real_score 0.755859, fake_score 0.0746743\n",
      "Epoch 101, Step 0: real_score 0.872461, fake_score 0.24747\n",
      "Epoch 101, Step 1000: real_score 0.877579, fake_score 0.179047\n",
      "Epoch 102, Step 0: real_score 0.890199, fake_score 0.239807\n",
      "Epoch 102, Step 1000: real_score 0.725472, fake_score 0.121706\n",
      "Epoch 103, Step 0: real_score 0.7972, fake_score 0.0566941\n",
      "Epoch 103, Step 1000: real_score 0.872859, fake_score 0.15585\n",
      "Epoch 104, Step 0: real_score 0.749816, fake_score 0.083632\n",
      "Epoch 104, Step 1000: real_score 0.853442, fake_score 0.171748\n",
      "Epoch 105, Step 0: real_score 0.738002, fake_score 0.0912908\n",
      "Epoch 105, Step 1000: real_score 0.894193, fake_score 0.100845\n",
      "Epoch 106, Step 0: real_score 0.873753, fake_score 0.209008\n",
      "Epoch 106, Step 1000: real_score 0.883996, fake_score 0.270038\n",
      "Epoch 107, Step 0: real_score 0.867213, fake_score 0.124524\n",
      "Epoch 107, Step 1000: real_score 0.781772, fake_score 0.1223\n",
      "Epoch 108, Step 0: real_score 0.831252, fake_score 0.182376\n",
      "Epoch 108, Step 1000: real_score 0.74262, fake_score 0.103519\n",
      "Epoch 109, Step 0: real_score 0.888584, fake_score 0.236452\n",
      "Epoch 109, Step 1000: real_score 0.866859, fake_score 0.134334\n",
      "Epoch 110, Step 0: real_score 0.829762, fake_score 0.170436\n",
      "Epoch 110, Step 1000: real_score 0.901834, fake_score 0.258225\n",
      "Epoch 111, Step 0: real_score 0.85331, fake_score 0.101227\n",
      "Epoch 111, Step 1000: real_score 0.766901, fake_score 0.231831\n",
      "Epoch 112, Step 0: real_score 0.858214, fake_score 0.182715\n",
      "Epoch 112, Step 1000: real_score 0.882597, fake_score 0.113473\n",
      "Epoch 113, Step 0: real_score 0.845083, fake_score 0.170905\n",
      "Epoch 113, Step 1000: real_score 0.850639, fake_score 0.16355\n",
      "Epoch 114, Step 0: real_score 0.874246, fake_score 0.230492\n",
      "Epoch 114, Step 1000: real_score 0.944321, fake_score 0.259003\n",
      "Epoch 115, Step 0: real_score 0.906426, fake_score 0.166386\n",
      "Epoch 115, Step 1000: real_score 0.738974, fake_score 0.108962\n",
      "Epoch 116, Step 0: real_score 0.887781, fake_score 0.231523\n",
      "Epoch 116, Step 1000: real_score 0.793845, fake_score 0.210637\n",
      "Epoch 117, Step 0: real_score 0.913191, fake_score 0.235695\n",
      "Epoch 117, Step 1000: real_score 0.785484, fake_score 0.200464\n",
      "Epoch 118, Step 0: real_score 0.833651, fake_score 0.253007\n",
      "Epoch 118, Step 1000: real_score 0.739845, fake_score 0.152183\n",
      "Epoch 119, Step 0: real_score 0.845943, fake_score 0.179801\n",
      "Epoch 119, Step 1000: real_score 0.840454, fake_score 0.176489\n",
      "Epoch 120, Step 0: real_score 0.76845, fake_score 0.126752\n",
      "Epoch 120, Step 1000: real_score 0.920574, fake_score 0.131515\n",
      "Epoch 121, Step 0: real_score 0.862293, fake_score 0.101155\n",
      "Epoch 121, Step 1000: real_score 0.809343, fake_score 0.205804\n",
      "Epoch 122, Step 0: real_score 0.838927, fake_score 0.210323\n",
      "Epoch 122, Step 1000: real_score 0.870655, fake_score 0.162077\n",
      "Epoch 123, Step 0: real_score 0.854636, fake_score 0.0952241\n",
      "Epoch 123, Step 1000: real_score 0.877067, fake_score 0.131828\n",
      "Epoch 124, Step 0: real_score 0.803502, fake_score 0.150808\n",
      "Epoch 124, Step 1000: real_score 0.715242, fake_score 0.133766\n",
      "Epoch 125, Step 0: real_score 0.704531, fake_score 0.116134\n",
      "Epoch 125, Step 1000: real_score 0.783066, fake_score 0.0822335\n",
      "Epoch 126, Step 0: real_score 0.84616, fake_score 0.269485\n",
      "Epoch 126, Step 1000: real_score 0.864165, fake_score 0.191889\n",
      "Epoch 127, Step 0: real_score 0.840881, fake_score 0.113683\n",
      "Epoch 127, Step 1000: real_score 0.939994, fake_score 0.274687\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 图片预处理\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist = torchvision.datasets.MNIST(root=r'D:\\_workPlace\\Book\\AI\\CV\\HandwrittenDigitRecognition\\MNIST_DATA',\n",
    "                                   train=True,\n",
    "                                   download=True,\n",
    "                                   transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "total_step = len(dataloader)\n",
    "D = Discriminator() # 不能和类同名\n",
    "G = Generator()\n",
    "# binary cross entropy 二分类交叉熵损失函数 https://www.jianshu.com/p/ac3bec3dde3e\n",
    "# https://juejin.im/post/6844903630479294477\n",
    "loss_func = nn.BCELoss()\n",
    "# Adam优化\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        #当dataloader的数量不是batch_size的整数倍时，应将最后一个batch_size改为实际的大小\n",
    "        BATCH_SIZE = images.size(0)\n",
    "        # [BATCH_SIZE, 28, 28] ---> [BATCH_SIZE, 28*28]\n",
    "        real_imgs = images.reshape(BATCH_SIZE, img_size).to(device)\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "        \n",
    "        # 鉴别真实图片\n",
    "        y_preds = D(real_imgs)\n",
    "        d_loss_real = loss_func(y_preds, real_labels)\n",
    "        real_score = y_preds\n",
    "        \n",
    "        # 生成假图片\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_imgs = G(z)\n",
    "        \n",
    "        # 鉴别假图片\n",
    "        y_preds = D(fake_imgs.detach()) # 采用detach截断，使得d_loss_fake方向传播不会流入生成器\n",
    "        d_loss_fake = loss_func(y_preds, fake_labels)\n",
    "        fake_score = y_preds\n",
    "        \n",
    "        # 优化Discrimator\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad() # 梯度清零，防止梯度累积\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 优化Generator\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_imgs = G(z)\n",
    "        y_preds = D(fake_imgs)\n",
    "        g_loss = loss_func(y_preds, real_labels) # 让D尽可能得将生成的图片判别为真图片\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i%1000==0:\n",
    "            print(\"Epoch %d, Step %d: real_score=%g, fake_score=%g\"%(epoch, i, real_score.mean().item(), fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ef6c14e388>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p6d1462f79e)\">\r\n    <image height=\"218\" id=\"imageff908f6bd2\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0ec56e7398\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m0ec56e7398\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc25e769c6c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc25e769c6c\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6d1462f79e\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# generate image\n",
    "import matplotlib.pyplot as plt\n",
    "z = torch.randn(1, latent_size).to(device)\n",
    "fake_imgs = G(z).view(28, 28).data.cpu().numpy()\n",
    "plt.imshow(fake_imgs, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model():\n",
    "    torch.save(D.state_dict(), 'GAN_Model/discriminator.pth')\n",
    "    torch.save(G.state_dict(), 'GAN_Model/generator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def load_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    D = Discriminator() # 不能和类同名\n",
    "    G = Generator()\n",
    "    D.to(device=device)\n",
    "    G.to(device=device)\n",
    "    D.load_state_dict(torch.load('GAN_Model/discriminator.pth', map_location=device))\n",
    "    G.load_state_dict(torch.load('GAN_Model/generator.pth', map_location=device))\n",
    "    return G, D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}